{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3f7d86-c945-408e-aec5-bc21f52b307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required libraries are installed successfully!\n",
      "============================================================\n",
      "ASSOCIATION RULE MINING PROJECT - DATA MINING MIDTERM\n",
      "============================================================\n",
      "Initializing Association Rule Mining Project...\n",
      "Initializing Association Rule Mining Project...\n",
      "\n",
      "============================================================\n",
      "CREATING 5 TRANSACTIONAL DATABASES\n",
      "============================================================\n",
      "✓ Initialized 5 companies with 15 items each\n",
      "Creating 50 meaningful transactions for Amazon...\n",
      "✓ Created amazon_transactions.csv with 50 transactions\n",
      "Creating 50 meaningful transactions for Walmart...\n",
      "✓ Created walmart_transactions.csv with 50 transactions\n",
      "Creating 50 meaningful transactions for Nike...\n",
      "✓ Created nike_transactions.csv with 50 transactions\n",
      "Creating 50 meaningful transactions for BestBuy...\n",
      "✓ Created bestbuy_transactions.csv with 50 transactions\n",
      "Creating 50 meaningful transactions for Target...\n",
      "✓ Created target_transactions.csv with 50 transactions\n",
      "\n",
      "✓ Successfully created 5 CSV databases\n",
      "✓ All databases are ready for association rule mining!\n",
      "============================================================\n",
      "Found 5 transaction databases\n",
      "\n",
      "============================================================\n",
      "ASSOCIATION RULE MINING ANALYSIS SYSTEM\n",
      "============================================================\n",
      "This system will:\n",
      "1. Show available databases\n",
      "2. Let you select ONE database\n",
      "3. Run Brute Force, Apriori, and FP-Growth algorithms\n",
      "4. Provide detailed performance comparisons\n",
      "\n",
      "RECOMMENDED PARAMETERS FOR MEANINGFUL RESULTS:\n",
      "Support: 0.05 to 0.3 (lower = more itemsets)\n",
      "Confidence: 0.3 to 0.8\n",
      "============================================================\n",
      "\n",
      "############################################################\n",
      "ANALYSIS SESSION #1\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "AVAILABLE DATABASES\n",
      "============================================================\n",
      "1. Amazon (50 transactions, 13 items)\n",
      "2. Bestbuy (50 transactions, 14 items)\n",
      "3. Nike (50 transactions, 13 items)\n",
      "4. Target (50 transactions, 15 items)\n",
      "5. Walmart (50 transactions, 13 items)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Select a database (1-5):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Amazon\n",
      "\n",
      "SET ANALYSIS PARAMETERS\n",
      "------------------------------\n",
      "RECOMMENDED RANGES:\n",
      "Support: 0.05 to 0.3 (lower = more itemsets)\n",
      "Confidence: 0.3 to 0.8\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter minimum support (0.01 to 1.0):  .3\n",
      "Enter minimum confidence (0.01 to 1.0):  .2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting analysis with:\n",
      "  Database: Amazon\n",
      "  Support: 0.3\n",
      "  Confidence: 0.2\n",
      "Running all algorithms...\n",
      "\n",
      "Running Brute Force Algorithm...\n",
      "  Transactions analyzed: 50\n",
      "  Support threshold: 0.3 (min 15 occurrences)\n",
      "  Found 2 frequent itemsets across 1 sizes\n",
      "  Generated 0 association rules\n",
      "  Example frequent itemsets:\n",
      "    {'electronics'} (support: 0.600)\n",
      "    {'home_kitchen'} (support: 0.400)\n",
      "Running Apriori Algorithm...\n",
      "  Found 2 frequent itemsets, 0 rules\n",
      "  Top 3 frequent itemsets:\n",
      "    ['electronics'] (support: 0.600)\n",
      "    ['home_kitchen'] (support: 0.400)\n",
      "Running FP-Growth Algorithm...\n",
      "  Found 2 frequent itemsets, 0 rules\n",
      "  Top 3 frequent itemsets:\n",
      "    ['electronics'] (support: 0.600)\n",
      "    ['home_kitchen'] (support: 0.400)\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE ALGORITHM COMPARISON REPORT\n",
      "================================================================================\n",
      "Dataset: Amazon\n",
      "Parameters: Support >= 0.3, Confidence >= 0.2\n",
      "================================================================================\n",
      "PERFORMANCE SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "Algorithm       Time                 Itemsets   Rules      Itemsets/sec   \n",
      "--------------------------------------------------------------------------------\n",
      "Brute Force     945.60 microseconds  2          0          2115.06        \n",
      "Apriori         16.12 milliseconds   2          0          124.05         \n",
      "FP-Growth       8.44 milliseconds    2          0          236.89         \n",
      "--------------------------------------------------------------------------------\n",
      "KEY FINDINGS:\n",
      "Fastest Algorithm: Brute Force (945.60 microseconds)\n",
      "Most Itemsets Found: Brute Force (2 itemsets)\n",
      "Most Rules Found: Brute Force (0 rules)\n",
      "Apriori is 0.1x faster than Brute Force\n",
      "FP-Growth is 1.9x faster than Apriori\n",
      "✓ All algorithms found the same number of itemsets - RESULTS CONSISTENT\n",
      "→ RECOMMENDATION: Brute Force is best for educational purposes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Run another analysis? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Thank you for using the Association Rule Mining System!\n",
      "============================================================\n",
      "Project execution completed!\n"
     ]
    }
   ],
   "source": [
    "# Data Mining Midterm Project - Association Rule Mining\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if required libraries are installed\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "    from mlxtend.frequent_patterns import association_rules\n",
    "    print(\"✓ All required libraries are installed successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Missing library: {e}\")\n",
    "    print(\"Please run the following installation commands:\")\n",
    "    print(\"pip install pandas numpy matplotlib seaborn mlxtend\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ASSOCIATION RULE MINING PROJECT - DATA MINING MIDTERM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Initializing Association Rule Mining Project...\")\n",
    "\n",
    "class CompanyDatasetCreator:\n",
    "    def __init__(self):\n",
    "        # Define items directly without external files\n",
    "        self.companies = {\n",
    "            'Amazon': [\n",
    "                'echo_dot', 'fire_tv', 'kindle', 'alexa_skills', 'prime_video',\n",
    "                'amazon_fresh', 'books', 'electronics', 'home_kitchen', 'toys',\n",
    "                'fashion', 'beauty', 'sports', 'garden', 'office_supplies'\n",
    "            ],\n",
    "            'Walmart': [\n",
    "                'groceries', 'clothing', 'electronics', 'home_decor', 'toys',\n",
    "                'pharmacy', 'automotive', 'sports_goods', 'furniture', 'jewelry',\n",
    "                'baby_products', 'pet_supplies', 'cleaning_supplies', 'garden', 'party_supplies'\n",
    "            ],\n",
    "            'Nike': [\n",
    "                'running_shoes', 'basketball_shoes', 'training_shoes', 'sneakers',\n",
    "                'athletic_shorts', 'sports_bras', 't_shirts', 'hoodies', 'jackets',\n",
    "                'leggings', 'socks', 'hats', 'backpacks', 'water_bottles', 'accessories'\n",
    "            ],\n",
    "            'BestBuy': [\n",
    "                'laptops', 'tvs', 'headphones', 'smartphones', 'tablets',\n",
    "                'gaming_consoles', 'cameras', 'smart_home', 'appliances', 'audio_systems',\n",
    "                'computer_parts', 'cables', 'printers', 'drones', 'wearables'\n",
    "            ],\n",
    "            'Target': [\n",
    "                'home_decor', 'clothing', 'electronics', 'toys', 'groceries',\n",
    "                'beauty', 'kitchen', 'furniture', 'seasonal', 'school_supplies',\n",
    "                'party_supplies', 'sports', 'baby', 'pet_supplies', 'health'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Verify we have exactly 5 databases as required\n",
    "        if len(self.companies) != 5:\n",
    "            raise ValueError(\"Must have exactly 5 databases as required by project specifications\")\n",
    "        \n",
    "        print(f\"✓ Initialized {len(self.companies)} companies with 15 items each\")\n",
    "\n",
    "    def generate_meaningful_transaction(self, items, company_name, transaction_id):\n",
    "        \"\"\"\n",
    "        Generate transactions with meaningful patterns that will create frequent itemsets\n",
    "        \"\"\"\n",
    "        # Create company-specific patterns that will generate frequent itemsets\n",
    "        company_patterns = {\n",
    "            'Amazon': [\n",
    "                ['electronics', 'home_kitchen', 'toys'],  # High frequency pattern\n",
    "                ['books', 'kindle', 'electronics'],       # Medium frequency  \n",
    "                ['amazon_fresh', 'groceries', 'home_kitchen'], # Medium frequency\n",
    "                ['echo_dot', 'alexa_skills', 'electronics'], # Low frequency\n",
    "                ['fashion', 'beauty', 'accessories']      # Low frequency\n",
    "            ],\n",
    "            'Walmart': [\n",
    "                ['groceries', 'cleaning_supplies', 'home_decor'], # High frequency\n",
    "                ['clothing', 'electronics', 'accessories'],       # High frequency\n",
    "                ['toys', 'baby_products', 'clothing'],           # Medium frequency\n",
    "                ['pharmacy', 'health', 'groceries'],             # Medium frequency\n",
    "                ['sports_goods', 'automotive', 'electronics']    # Low frequency\n",
    "            ],\n",
    "            'Nike': [\n",
    "                ['running_shoes', 'athletic_shorts', 't_shirts'], # High frequency\n",
    "                ['basketball_shoes', 't_shirts', 'socks'],       # High frequency\n",
    "                ['hoodies', 'leggings', 'sports_bras'],          # Medium frequency\n",
    "                ['sports_bras', 'training_shoes', 'athletic_shorts'], # Medium frequency\n",
    "                ['sneakers', 'socks', 'hats']                    # Low frequency\n",
    "            ],\n",
    "            'BestBuy': [\n",
    "                ['electronics', 'laptops', 'computer_parts'],    # High frequency\n",
    "                ['tvs', 'audio_systems', 'electronics'],         # High frequency\n",
    "                ['smartphones', 'headphones', 'electronics'],    # Medium frequency\n",
    "                ['gaming_consoles', 'electronics', 'tvs'],       # Medium frequency\n",
    "                ['cameras', 'accessories', 'drones']             # Low frequency\n",
    "            ],\n",
    "            'Target': [\n",
    "                ['home_decor', 'kitchen', 'furniture'],          # High frequency\n",
    "                ['clothing', 'electronics', 'accessories'],      # High frequency\n",
    "                ['groceries', 'cleaning_supplies', 'home_decor'], # Medium frequency\n",
    "                ['toys', 'baby', 'clothing'],                    # Medium frequency\n",
    "                ['school_supplies', 'office', 'electronics']     # Low frequency\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Determine transaction pattern based on deterministic selection\n",
    "        pattern_index = (sum(ord(c) for c in company_name) + transaction_id) % len(company_patterns[company_name])\n",
    "        base_pattern = company_patterns[company_name][pattern_index]\n",
    "        \n",
    "        # Add some variation while keeping core patterns\n",
    "        transaction = list(base_pattern)\n",
    "        \n",
    "        # Occasionally add extra items to create more combinations (30% of transactions)\n",
    "        if (transaction_id % 3) == 0:\n",
    "            available_extras = [item for item in items if item not in transaction]\n",
    "            if available_extras:\n",
    "                extra_index = transaction_id % len(available_extras)\n",
    "                transaction.append(available_extras[extra_index])\n",
    "        \n",
    "        # Ensure all items in transaction exist in our item list\n",
    "        transaction = [item for item in transaction if item in items]\n",
    "        \n",
    "        return sorted(transaction)\n",
    "\n",
    "    def create_dataset(self, company_name, num_transactions=50): \n",
    "        \"\"\"Create dataset with meaningful patterns that generate frequent itemsets\"\"\"\n",
    "        if company_name not in self.companies:\n",
    "            raise ValueError(f\"Unknown company: {company_name}\")\n",
    "            \n",
    "        items = self.companies[company_name]\n",
    "        if not items:\n",
    "            raise ValueError(f\"No items available for {company_name}\")\n",
    "            \n",
    "        transactions = []\n",
    "        \n",
    "        print(f\"Creating {num_transactions} meaningful transactions for {company_name}...\")\n",
    "        \n",
    "        for i in range(num_transactions):\n",
    "            transaction_items = self.generate_meaningful_transaction(items, company_name, i)\n",
    "            transactions.append({\n",
    "                'transaction_id': i + 1,\n",
    "                'items': ','.join(transaction_items)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(transactions)\n",
    "\n",
    "class BruteForceMiner:\n",
    "    def __init__(self, min_support=0.1, min_confidence=0.5):\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.frequent_itemsets = {}\n",
    "    \n",
    "    def load_transactions(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        transactions = []\n",
    "        for _, row in df.iterrows():\n",
    "            items = row['items'].split(',')\n",
    "            transactions.append(frozenset(items))\n",
    "        return transactions\n",
    "    \n",
    "    def get_all_items(self, transactions):\n",
    "        all_items = set()\n",
    "        for transaction in transactions:\n",
    "            all_items.update(transaction)\n",
    "        return sorted(all_items)\n",
    "    \n",
    "    def calculate_support(self, itemset, transactions):\n",
    "        count = 0\n",
    "        for transaction in transactions:\n",
    "            if itemset.issubset(transaction):\n",
    "                count += 1\n",
    "        return count / len(transactions)\n",
    "    \n",
    "    def generate_k_itemsets(self, items, k):\n",
    "        return [frozenset(combo) for combo in combinations(items, k)]\n",
    "    \n",
    "    def find_frequent_itemsets(self, transactions):\n",
    "        all_items = self.get_all_items(transactions)\n",
    "        self.frequent_itemsets = {}\n",
    "        \n",
    "        k = 1\n",
    "        while True:\n",
    "            candidate_itemsets = self.generate_k_itemsets(all_items, k)\n",
    "            frequent_k_itemsets = []\n",
    "            \n",
    "            for itemset in candidate_itemsets:\n",
    "                support = self.calculate_support(itemset, transactions)\n",
    "                if support >= self.min_support:\n",
    "                    frequent_k_itemsets.append((itemset, support))\n",
    "            \n",
    "            if not frequent_k_itemsets:\n",
    "                break\n",
    "                \n",
    "            self.frequent_itemsets[k] = frequent_k_itemsets\n",
    "            k += 1\n",
    "        \n",
    "        return self.frequent_itemsets\n",
    "    \n",
    "    def generate_association_rules(self, transactions):\n",
    "        rules = []\n",
    "        \n",
    "        for k in range(2, len(self.frequent_itemsets) + 1):\n",
    "            for itemset, support in self.frequent_itemsets[k]:\n",
    "                itemset_list = list(itemset)\n",
    "                \n",
    "                for i in range(1, len(itemset_list)):\n",
    "                    for antecedent in combinations(itemset_list, i):\n",
    "                        antecedent_set = frozenset(antecedent)\n",
    "                        consequent_set = itemset - antecedent_set\n",
    "                        \n",
    "                        antecedent_support = self.calculate_support(antecedent_set, transactions)\n",
    "                        if antecedent_support > 0:\n",
    "                            confidence = support / antecedent_support\n",
    "                            \n",
    "                            if confidence >= self.min_confidence:\n",
    "                                rules.append({\n",
    "                                    'antecedent': antecedent_set,\n",
    "                                    'consequent': consequent_set,\n",
    "                                    'support': support,\n",
    "                                    'confidence': confidence\n",
    "                                })\n",
    "        \n",
    "        return rules\n",
    "\n",
    "class LibraryMiner:\n",
    "    def __init__(self, min_support=0.1, min_confidence=0.5):\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "    \n",
    "    def load_transactions(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        transactions = []\n",
    "        for _, row in df.iterrows():\n",
    "            transactions.append(row['items'].split(','))\n",
    "        return transactions\n",
    "    \n",
    "    def prepare_data(self, transactions):\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        return pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    def run_apriori(self, encoded_df):\n",
    "        frequent_itemsets = apriori(encoded_df, min_support=self.min_support, use_colnames=True)\n",
    "        if len(frequent_itemsets) > 0:\n",
    "            rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=self.min_confidence)\n",
    "        else:\n",
    "            rules = pd.DataFrame()\n",
    "        return frequent_itemsets, rules\n",
    "    \n",
    "    def run_fpgrowth(self, encoded_df):\n",
    "        frequent_itemsets = fpgrowth(encoded_df, min_support=self.min_support, use_colnames=True)\n",
    "        if len(frequent_itemsets) > 0:\n",
    "            rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=self.min_confidence)\n",
    "        else:\n",
    "            rules = pd.DataFrame()\n",
    "        return frequent_itemsets, rules\n",
    "\n",
    "class CompleteAnalysisSystem:\n",
    "    def __init__(self):\n",
    "        self.datasets = self.find_datasets()\n",
    "        self.bf_miner = BruteForceMiner()\n",
    "        \n",
    "        # Verify we have exactly 5 databases\n",
    "        if len(self.datasets) != 5:\n",
    "            print(f\"WARNING: Expected 5 databases, found {len(self.datasets)}\")\n",
    "    \n",
    "    def find_datasets(self):\n",
    "        datasets = {}\n",
    "        csv_files = [f for f in os.listdir('.') if f.endswith('_transactions.csv')]\n",
    "        \n",
    "        for file in csv_files:\n",
    "            company = file.replace('_transactions.csv', '').title()\n",
    "            datasets[company] = file\n",
    "        \n",
    "        print(f\"Found {len(datasets)} transaction databases\")\n",
    "        return datasets\n",
    "    \n",
    "    def display_menu(self):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"AVAILABLE DATABASES\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, (company, filename) in enumerate(self.datasets.items(), 1):\n",
    "            df = pd.read_csv(filename)\n",
    "            all_items = set()\n",
    "            for items_str in df['items']:\n",
    "                all_items.update(items_str.split(','))\n",
    "            print(f\"{i}. {company} ({len(df)} transactions, {len(all_items)} items)\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def get_user_parameters(self):\n",
    "        print(\"\\nSET ANALYSIS PARAMETERS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        print(\"RECOMMENDED RANGES:\")\n",
    "        print(\"Support: 0.05 to 0.3 (lower = more itemsets)\")\n",
    "        print(\"Confidence: 0.3 to 0.8\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                support = input(\"Enter minimum support (0.01 to 1.0): \").strip()\n",
    "                support_val = float(support)\n",
    "                if 0.01 <= support_val <= 1.0:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Support must be between 0.01 and 1.0\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number (e.g., 0.1)\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                confidence = input(\"Enter minimum confidence (0.01 to 1.0): \").strip()\n",
    "                confidence_val = float(confidence)\n",
    "                if 0.01 <= confidence_val <= 1.0:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Confidence must be between 0.01 and 1.0\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number (e.g., 0.5)\")\n",
    "        \n",
    "        return support_val, confidence_val\n",
    "    \n",
    "    def get_dataset_choice(self):\n",
    "        while True:\n",
    "            try:\n",
    "                choice = input(f\"\\nSelect a database (1-{len(self.datasets)}): \").strip()\n",
    "                choice_num = int(choice)\n",
    "                \n",
    "                if 1 <= choice_num <= len(self.datasets):\n",
    "                    companies = list(self.datasets.keys())\n",
    "                    selected = companies[choice_num - 1]\n",
    "                    print(f\"Selected: {selected}\")\n",
    "                    return selected\n",
    "                else:\n",
    "                    print(f\"Please enter a number between 1 and {len(self.datasets)}\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "    \n",
    "    def run_brute_force(self, company, support, confidence):\n",
    "        print(\"\\nRunning Brute Force Algorithm...\")\n",
    "        filename = self.datasets[company]\n",
    "        \n",
    "        self.bf_miner.min_support = support\n",
    "        self.bf_miner.min_confidence = confidence\n",
    "        \n",
    "        transactions = self.bf_miner.load_transactions(filename)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        frequent_itemsets = self.bf_miner.find_frequent_itemsets(transactions)\n",
    "        rules = self.bf_miner.generate_association_rules(transactions)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        itemset_count = sum(len(itemsets) for itemsets in frequent_itemsets.values())\n",
    "        \n",
    "        # Show detailed results\n",
    "        print(f\"  Transactions analyzed: {len(transactions)}\")\n",
    "        print(f\"  Support threshold: {support} (min {int(support * len(transactions))} occurrences)\")\n",
    "        print(f\"  Found {itemset_count} frequent itemsets across {len(frequent_itemsets)} sizes\")\n",
    "        print(f\"  Generated {len(rules)} association rules\")\n",
    "        \n",
    "        # Show some example itemsets if found\n",
    "        if itemset_count > 0:\n",
    "            print(\"  Example frequent itemsets:\")\n",
    "            for k, itemsets in list(frequent_itemsets.items())[:2]:  # Show first 2 sizes\n",
    "                for itemset, supp in itemsets[:3]:  # Show first 3 of each size\n",
    "                    print(f\"    {set(itemset)} (support: {supp:.3f})\")\n",
    "        \n",
    "        return {\n",
    "            'time': end_time - start_time,\n",
    "            'itemsets': itemset_count,\n",
    "            'rules': len(rules),\n",
    "            'frequent_itemsets': frequent_itemsets,\n",
    "            'association_rules': rules\n",
    "        }\n",
    "    \n",
    "    def run_apriori(self, company, support, confidence):\n",
    "        print(\"Running Apriori Algorithm...\")\n",
    "        filename = self.datasets[company]\n",
    "        \n",
    "        df = pd.read_csv(filename)\n",
    "        transactions = []\n",
    "        for _, row in df.iterrows():\n",
    "            transactions.append(row['items'].split(','))\n",
    "        \n",
    "        te = TransactionEncoder()\n",
    "        encoded_df = pd.DataFrame(te.fit_transform(transactions), columns=te.columns_)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        itemsets = apriori(encoded_df, min_support=support, use_colnames=True)\n",
    "        if len(itemsets) > 0:\n",
    "            rules = association_rules(itemsets, metric=\"confidence\", min_threshold=confidence)\n",
    "        else:\n",
    "            rules = pd.DataFrame()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        print(f\"  Found {len(itemsets)} frequent itemsets, {len(rules)} rules\")\n",
    "        \n",
    "        # Show some example itemsets if found\n",
    "        if len(itemsets) > 0:\n",
    "            print(\"  Top 3 frequent itemsets:\")\n",
    "            for _, row in itemsets.head(3).iterrows():\n",
    "                itemset = list(row['itemsets'])\n",
    "                print(f\"    {itemset} (support: {row['support']:.3f})\")\n",
    "        \n",
    "        return {\n",
    "            'time': end_time - start_time,\n",
    "            'itemsets': len(itemsets),\n",
    "            'rules': len(rules)\n",
    "        }\n",
    "    \n",
    "    def run_fpgrowth(self, company, support, confidence):\n",
    "        print(\"Running FP-Growth Algorithm...\")\n",
    "        filename = self.datasets[company]\n",
    "        \n",
    "        df = pd.read_csv(filename)\n",
    "        transactions = []\n",
    "        for _, row in df.iterrows():\n",
    "            transactions.append(row['items'].split(','))\n",
    "        \n",
    "        te = TransactionEncoder()\n",
    "        encoded_df = pd.DataFrame(te.fit_transform(transactions), columns=te.columns_)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        itemsets = fpgrowth(encoded_df, min_support=support, use_colnames=True)\n",
    "        if len(itemsets) > 0:\n",
    "            rules = association_rules(itemsets, metric=\"confidence\", min_threshold=confidence)\n",
    "        else:\n",
    "            rules = pd.DataFrame()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        print(f\"  Found {len(itemsets)} frequent itemsets, {len(rules)} rules\")\n",
    "        \n",
    "        # Show some example itemsets if found\n",
    "        if len(itemsets) > 0:\n",
    "            print(\"  Top 3 frequent itemsets:\")\n",
    "            for _, row in itemsets.head(3).iterrows():\n",
    "                itemset = list(row['itemsets'])\n",
    "                print(f\"    {itemset} (support: {row['support']:.3f})\")\n",
    "        \n",
    "        return {\n",
    "            'time': end_time - start_time,\n",
    "            'itemsets': len(itemsets),\n",
    "            'rules': len(rules)\n",
    "        }\n",
    "    \n",
    "    def safe_division(self, numerator, denominator):\n",
    "        min_denominator = 0.000001\n",
    "        safe_denominator = max(denominator, min_denominator)\n",
    "        return numerator / safe_denominator\n",
    "    \n",
    "    def format_time(self, seconds):\n",
    "        if seconds < 0.001:\n",
    "            return f\"{seconds * 1000000:.2f} microseconds\"\n",
    "        elif seconds < 1.0:\n",
    "            return f\"{seconds * 1000:.2f} milliseconds\"\n",
    "        else:\n",
    "            return f\"{seconds:.4f} seconds\"\n",
    "    \n",
    "    def generate_comparison_report(self, results, company, support, confidence):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"COMPREHENSIVE ALGORITHM COMPARISON REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Dataset: {company}\")\n",
    "        print(f\"Parameters: Support >= {support}, Confidence >= {confidence}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"PERFORMANCE SUMMARY:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Algorithm':<15} {'Time':<20} {'Itemsets':<10} {'Rules':<10} {'Itemsets/sec':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for algo_name, result in results.items():\n",
    "            time_formatted = self.format_time(result['time'])\n",
    "            itemsets_per_sec = self.safe_division(result['itemsets'], result['time'])\n",
    "            print(f\"{algo_name:<15} {time_formatted:<20} {result['itemsets']:<10} {result['rules']:<10} {itemsets_per_sec:<15.2f}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Filter out algorithms with 0 itemsets for meaningful comparisons\n",
    "        non_zero_results = {k: v for k, v in results.items() if v['itemsets'] > 0}\n",
    "        \n",
    "        if non_zero_results:\n",
    "            fastest_algo = min(non_zero_results.items(), key=lambda x: x[1]['time'])\n",
    "            most_itemsets = max(non_zero_results.items(), key=lambda x: x[1]['itemsets'])\n",
    "            most_rules = max(non_zero_results.items(), key=lambda x: x[1]['rules'])\n",
    "            \n",
    "            print(\"KEY FINDINGS:\")\n",
    "            print(f\"Fastest Algorithm: {fastest_algo[0]} ({self.format_time(fastest_algo[1]['time'])})\")\n",
    "            print(f\"Most Itemsets Found: {most_itemsets[0]} ({most_itemsets[1]['itemsets']} itemsets)\")\n",
    "            print(f\"Most Rules Found: {most_rules[0]} ({most_rules[1]['rules']} rules)\")\n",
    "            \n",
    "            # Performance comparisons\n",
    "            bf_result = results.get('Brute Force', {})\n",
    "            apriori_result = results.get('Apriori', {})\n",
    "            fpgrowth_result = results.get('FP-Growth', {})\n",
    "            \n",
    "            if bf_result.get('itemsets', 0) > 0 and apriori_result.get('itemsets', 0) > 0:\n",
    "                speedup = self.safe_division(bf_result['time'], apriori_result['time'])\n",
    "                print(f\"Apriori is {speedup:.1f}x faster than Brute Force\")\n",
    "            \n",
    "            if fpgrowth_result.get('itemsets', 0) > 0 and apriori_result.get('itemsets', 0) > 0:\n",
    "                if fpgrowth_result['time'] < apriori_result['time']:\n",
    "                    speedup = self.safe_division(apriori_result['time'], fpgrowth_result['time'])\n",
    "                    print(f\"FP-Growth is {speedup:.1f}x faster than Apriori\")\n",
    "                else:\n",
    "                    speedup = self.safe_division(fpgrowth_result['time'], apriori_result['time'])\n",
    "                    print(f\"Apriori is {speedup:.1f}x faster than FP-Growth\")\n",
    "        else:\n",
    "            print(\"KEY FINDINGS:\")\n",
    "            print(\"No frequent itemsets found with current parameters.\")\n",
    "            print(\"Try lowering the support threshold (e.g., 0.05-0.2)\")\n",
    "        \n",
    "        # Consistency check\n",
    "        itemset_counts = [result['itemsets'] for result in results.values()]\n",
    "        if len(set(itemset_counts)) == 1:\n",
    "            if itemset_counts[0] > 0:\n",
    "                print(\"✓ All algorithms found the same number of itemsets - RESULTS CONSISTENT\")\n",
    "            else:\n",
    "                print(\"⚠ No algorithms found any frequent itemsets\")\n",
    "                print(\"💡 TIP: Try support=0.05-0.2 and confidence=0.3-0.7\")\n",
    "        else:\n",
    "            print(\"⚠ Algorithms found different numbers of itemsets\")\n",
    "            for algo_name, result in results.items():\n",
    "                print(f\"  {algo_name}: {result['itemsets']} itemsets\")\n",
    "        \n",
    "        # Recommendations\n",
    "        if non_zero_results:\n",
    "            fastest_algo_name = min(non_zero_results.items(), key=lambda x: x[1]['time'])[0]\n",
    "            if fastest_algo_name == 'FP-Growth':\n",
    "                print(\"→ RECOMMENDATION: FP-Growth for optimal performance\")\n",
    "            elif fastest_algo_name == 'Apriori':\n",
    "                print(\"→ RECOMMENDATION: Apriori provides good balance of speed and readability\")\n",
    "            else:\n",
    "                print(\"→ RECOMMENDATION: Brute Force is best for educational purposes\")\n",
    "        else:\n",
    "            print(\"→ RECOMMENDATION: Try support = 0.05-0.2 and confidence = 0.3-0.7\")\n",
    "    \n",
    "    def get_continue_choice(self):\n",
    "        while True:\n",
    "            choice = input(\"\\nRun another analysis? (y/n): \").strip().lower()\n",
    "            if choice in ['y', 'yes']:\n",
    "                return True\n",
    "            elif choice in ['n', 'no']:\n",
    "                return False\n",
    "            else:\n",
    "                print(\"Please enter 'y' for yes or 'n' for no\")\n",
    "    \n",
    "    def start_interactive_analysis(self):\n",
    "        if not self.datasets:\n",
    "            print(\"ERROR: No datasets found! Please check that datasets were created successfully.\")\n",
    "            return\n",
    "        \n",
    "        if len(self.datasets) != 5:\n",
    "            print(f\"WARNING: Expected 5 databases, but found {len(self.datasets)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ASSOCIATION RULE MINING ANALYSIS SYSTEM\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"This system will:\")\n",
    "        print(\"1. Show available databases\")\n",
    "        print(\"2. Let you select ONE database\")\n",
    "        print(\"3. Run Brute Force, Apriori, and FP-Growth algorithms\")\n",
    "        print(\"4. Provide detailed performance comparisons\")\n",
    "        print(\"\\nRECOMMENDED PARAMETERS FOR MEANINGFUL RESULTS:\")\n",
    "        print(\"Support: 0.05 to 0.3 (lower = more itemsets)\")\n",
    "        print(\"Confidence: 0.3 to 0.8\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        analysis_count = 0\n",
    "        \n",
    "        while True:\n",
    "            analysis_count += 1\n",
    "            print(\"\\n\" + \"#\" * 60)\n",
    "            print(f\"ANALYSIS SESSION #{analysis_count}\")\n",
    "            print(\"#\" * 60)\n",
    "            \n",
    "            self.display_menu()\n",
    "            \n",
    "            company = self.get_dataset_choice()\n",
    "            support, confidence = self.get_user_parameters()\n",
    "            \n",
    "            print(f\"\\nStarting analysis with:\")\n",
    "            print(f\"  Database: {company}\")\n",
    "            print(f\"  Support: {support}\")\n",
    "            print(f\"  Confidence: {confidence}\")\n",
    "            print(\"Running all algorithms...\")\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            results['Brute Force'] = self.run_brute_force(company, support, confidence)\n",
    "            results['Apriori'] = self.run_apriori(company, support, confidence)\n",
    "            results['FP-Growth'] = self.run_fpgrowth(company, support, confidence)\n",
    "            \n",
    "            self.generate_comparison_report(results, company, support, confidence)\n",
    "            \n",
    "            if not self.get_continue_choice():\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(\"Thank you for using the Association Rule Mining System!\")\n",
    "                print(\"=\" * 60)\n",
    "                break\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing Association Rule Mining Project...\")\n",
    "    \n",
    "    # Create datasets\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"CREATING 5 TRANSACTIONAL DATABASES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        creator = CompanyDatasetCreator()\n",
    "        \n",
    "        created_files = []\n",
    "        for company in creator.companies.keys():\n",
    "            df = creator.create_dataset(company, 50)  # Increased to 50 transactions\n",
    "            filename = f\"{company.lower()}_transactions.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            created_files.append(filename)\n",
    "            print(f\"✓ Created {filename} with {len(df)} transactions\")\n",
    "        \n",
    "        print(f\"\\n✓ Successfully created {len(created_files)} CSV databases\")\n",
    "        print(\"✓ All databases are ready for association rule mining!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating datasets: {e}\")\n",
    "    \n",
    "    # Run analysis system\n",
    "    try:\n",
    "        analysis_system = CompleteAnalysisSystem()\n",
    "        analysis_system.start_interactive_analysis()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "    \n",
    "    print(\"Project execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b88870-c74d-4a4a-8294-41d50186baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "✓ All required libraries are installed successfully!\n",
    "============================================================\n",
    "ASSOCIATION RULE MINING PROJECT - DATA MINING MIDTERM\n",
    "============================================================\n",
    "Initializing Association Rule Mining Project...\n",
    "Initializing Association Rule Mining Project...\n",
    "\n",
    "============================================================\n",
    "CREATING 5 TRANSACTIONAL DATABASES\n",
    "============================================================\n",
    "✓ Initialized 5 companies with 15 items each\n",
    "Creating 50 meaningful transactions for Amazon...\n",
    "✓ Created amazon_transactions.csv with 50 transactions\n",
    "Creating 50 meaningful transactions for Walmart...\n",
    "✓ Created walmart_transactions.csv with 50 transactions\n",
    "Creating 50 meaningful transactions for Nike...\n",
    "✓ Created nike_transactions.csv with 50 transactions\n",
    "Creating 50 meaningful transactions for BestBuy...\n",
    "✓ Created bestbuy_transactions.csv with 50 transactions\n",
    "Creating 50 meaningful transactions for Target...\n",
    "✓ Created target_transactions.csv with 50 transactions\n",
    "\n",
    "✓ Successfully created 5 CSV databases\n",
    "✓ All databases are ready for association rule mining!\n",
    "============================================================\n",
    "Found 5 transaction databases\n",
    "\n",
    "============================================================\n",
    "ASSOCIATION RULE MINING ANALYSIS SYSTEM\n",
    "============================================================\n",
    "This system will:\n",
    "1. Show available databases\n",
    "2. Let you select ONE database\n",
    "3. Run Brute Force, Apriori, and FP-Growth algorithms\n",
    "4. Provide detailed performance comparisons\n",
    "\n",
    "RECOMMENDED PARAMETERS FOR MEANINGFUL RESULTS:\n",
    "Support: 0.05 to 0.3 (lower = more itemsets)\n",
    "Confidence: 0.3 to 0.8\n",
    "============================================================\n",
    "\n",
    "############################################################\n",
    "ANALYSIS SESSION #1\n",
    "############################################################\n",
    "\n",
    "============================================================\n",
    "AVAILABLE DATABASES\n",
    "============================================================\n",
    "1. Amazon (50 transactions, 13 items)\n",
    "2. Bestbuy (50 transactions, 14 items)\n",
    "3. Nike (50 transactions, 13 items)\n",
    "4. Target (50 transactions, 15 items)\n",
    "5. Walmart (50 transactions, 13 items)\n",
    "============================================================\n",
    "\n",
    "Select a database (1-5):  1\n",
    "Selected: Amazon\n",
    "\n",
    "SET ANALYSIS PARAMETERS\n",
    "------------------------------\n",
    "RECOMMENDED RANGES:\n",
    "Support: 0.05 to 0.3 (lower = more itemsets)\n",
    "Confidence: 0.3 to 0.8\n",
    "------------------------------\n",
    "Enter minimum support (0.01 to 1.0):  .3\n",
    "Enter minimum confidence (0.01 to 1.0):  .2\n",
    "\n",
    "Starting analysis with:\n",
    "  Database: Amazon\n",
    "  Support: 0.3\n",
    "  Confidence: 0.2\n",
    "Running all algorithms...\n",
    "\n",
    "Running Brute Force Algorithm...\n",
    "  Transactions analyzed: 50\n",
    "  Support threshold: 0.3 (min 15 occurrences)\n",
    "  Found 2 frequent itemsets across 1 sizes\n",
    "  Generated 0 association rules\n",
    "  Example frequent itemsets:\n",
    "    {'electronics'} (support: 0.600)\n",
    "    {'home_kitchen'} (support: 0.400)\n",
    "Running Apriori Algorithm...\n",
    "  Found 2 frequent itemsets, 0 rules\n",
    "  Top 3 frequent itemsets:\n",
    "    ['electronics'] (support: 0.600)\n",
    "    ['home_kitchen'] (support: 0.400)\n",
    "Running FP-Growth Algorithm...\n",
    "  Found 2 frequent itemsets, 0 rules\n",
    "  Top 3 frequent itemsets:\n",
    "    ['electronics'] (support: 0.600)\n",
    "    ['home_kitchen'] (support: 0.400)\n",
    "\n",
    "================================================================================\n",
    "COMPREHENSIVE ALGORITHM COMPARISON REPORT\n",
    "================================================================================\n",
    "Dataset: Amazon\n",
    "Parameters: Support >= 0.3, Confidence >= 0.2\n",
    "================================================================================\n",
    "PERFORMANCE SUMMARY:\n",
    "--------------------------------------------------------------------------------\n",
    "Algorithm       Time                 Itemsets   Rules      Itemsets/sec   \n",
    "--------------------------------------------------------------------------------\n",
    "Brute Force     945.60 microseconds  2          0          2115.06        \n",
    "Apriori         16.12 milliseconds   2          0          124.05         \n",
    "FP-Growth       8.44 milliseconds    2          0          236.89         \n",
    "--------------------------------------------------------------------------------\n",
    "KEY FINDINGS:\n",
    "Fastest Algorithm: Brute Force (945.60 microseconds)\n",
    "Most Itemsets Found: Brute Force (2 itemsets)\n",
    "Most Rules Found: Brute Force (0 rules)\n",
    "Apriori is 0.1x faster than Brute Force\n",
    "FP-Growth is 1.9x faster than Apriori\n",
    "✓ All algorithms found the same number of itemsets - RESULTS CONSISTENT\n",
    "→ RECOMMENDATION: Brute Force is best for educational purposes\n",
    "\n",
    "Run another analysis? (y/n):  n\n",
    "\n",
    "============================================================\n",
    "Thank you for using the Association Rule Mining System!\n",
    "============================================================\n",
    "Project execution completed!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
